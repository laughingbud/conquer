{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Demo_PurgedKFoldCV.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laughingbud/conquer/blob/main/Demo_PurgedKFoldCV.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Combinatorial Purged Cross-Validation method: indexing example on crypto\n",
        "\n",
        "*By Berend Gort*\n",
        "\n",
        "www.medium.com/@CoderBurnt\n",
        "\n",
        "www.linkedin.com/in/berendgort/\n",
        "\n",
        "www.twitter.com/CoderBurnt\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KFmER7NGbpgZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Packages"
      ],
      "metadata": {
        "id": "RXozZT8vckj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "\n",
        "%cd /\n",
        "!git clone https://github.com/AI4Finance-Foundation/FinRL-Meta\n",
        "%cd /FinRL-Meta/\n",
        "!pip install git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
        "!pip install git+https://github.com/AI4Finance-LLC/FinRL-Library.git\n",
        "!pip install gputil\n",
        "!pip install trading_calendars\n",
        "!pip install fracdiff\n",
        "!pip install timeseriescv\n",
        "\n",
        "#install TA-lib (technical analysis)\n",
        "!wget http://prdownloads.sourceforge.net/ta-lib/ta-lib-0.4.0-src.tar.gz\n",
        "!tar xvzf ta-lib-0.4.0-src.tar.gz\n",
        "import os\n",
        "os.chdir('ta-lib')\n",
        "!./configure --prefix=/usr\n",
        "!make\n",
        "!make install\n",
        "os.chdir('../')\n",
        "!pip install TA-Lib\n",
        "!pip install python-binance"
      ],
      "metadata": {
        "id": "RQGyziIBbmpk",
        "outputId": "15548c02-1514-400b-eba7-39bc6e076e5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/\n",
            "Cloning into 'FinRL-Meta'...\n",
            "remote: Enumerating objects: 7993, done.\u001b[K\n",
            "remote: Counting objects: 100% (232/232), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 7993 (delta 113), reused 188 (delta 86), pack-reused 7761\u001b[K\n",
            "Receiving objects: 100% (7993/7993), 170.72 MiB | 8.68 MiB/s, done.\n",
            "Resolving deltas: 100% (4631/4631), done.\n",
            "Updating files: 100% (423/423), done.\n",
            "/FinRL-Meta\n",
            "Collecting git+https://github.com/AI4Finance-LLC/ElegantRL.git\n",
            "  Cloning https://github.com/AI4Finance-LLC/ElegantRL.git to /tmp/pip-req-build-cmypfyyj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-LLC/ElegantRL.git /tmp/pip-req-build-cmypfyyj\n",
            "  Resolved https://github.com/AI4Finance-LLC/ElegantRL.git to commit 155f07fcfe2d0f0a0318f820e8e2f2401ff30eca\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from ElegantRL==0.3.7) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ElegantRL==0.3.7) (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from ElegantRL==0.3.7) (3.7.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from ElegantRL==0.3.7) (0.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym->ElegantRL==0.3.7) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym->ElegantRL==0.3.7) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym->ElegantRL==0.3.7)\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym->ElegantRL==0.3.7)\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting swig==4.* (from gym->ElegantRL==0.3.7)\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->ElegantRL==0.3.7) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.7) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.7) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.7) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.7) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.7) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.7) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->ElegantRL==0.3.7)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->ElegantRL==0.3.7) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->ElegantRL==0.3.7)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->ElegantRL==0.3.7) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->ElegantRL==0.3.7) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->ElegantRL==0.3.7) (1.3.0)\n",
            "Building wheels for collected packages: ElegantRL, box2d-py\n",
            "  Building wheel for ElegantRL (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ElegantRL: filename=ElegantRL-0.3.7-py3-none-any.whl size=197301 sha256=fd124b1cff2ccbefc01c200da2e50935a0290f14625c2847e280ab35f4ee0ad7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-obamulyw/wheels/85/a7/e7/37369f52de5c5e77ba80ec8d0dd898ef99fbb23707e77a3958\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for box2d-py\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for box2d-py\n",
            "Successfully built ElegantRL\n",
            "Failed to build box2d-py\n",
            "\u001b[31mERROR: Could not build wheels for box2d-py, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting git+https://github.com/AI4Finance-LLC/FinRL-Library.git\n",
            "  Cloning https://github.com/AI4Finance-LLC/FinRL-Library.git to /tmp/pip-req-build-3ryqq4qs\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-LLC/FinRL-Library.git /tmp/pip-req-build-3ryqq4qs\n",
            "  Resolved https://github.com/AI4Finance-LLC/FinRL-Library.git to commit ad1e019b3c111af31aa15fc6f015652e77e9ec13\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-7pvmj62b/elegantrl_80a0d4b89ca541869b917988fe350f8e\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-7pvmj62b/elegantrl_80a0d4b89ca541869b917988fe350f8e\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 155f07fcfe2d0f0a0318f820e8e2f2401ff30eca\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alpaca-trade-api<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Collecting ccxt<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exchange-calendars<5,>=4 (from finrl==0.3.6)\n",
            "  Downloading exchange_calendars-4.5.4-py3-none-any.whl (192 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.6/192.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jqdatasdk<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading jqdatasdk-1.9.4-py3-none-any.whl (74 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.9/74.9 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyfolio<0.10,>=0.9 (from finrl==0.3.6)\n",
            "  Downloading pyfolio-0.9.2.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyportfolioopt<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading pyportfolioopt-1.5.5-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ray[default,tune]<3,>=2 (from finrl==0.3.6)\n",
            "  Downloading ray-2.23.0-cp310-cp310-manylinux2014_x86_64.whl (65.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (1.2.2)\n",
            "Collecting stable-baselines3[extra]>=2.0.0a5 (from finrl==0.3.6)\n",
            "  Downloading stable_baselines3-2.4.0a1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.3/182.3 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting stockstats<0.6,>=0.5 (from finrl==0.3.6)\n",
            "  Downloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
            "Collecting wrds<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading wrds-3.2.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: yfinance<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from finrl==0.3.6) (0.2.40)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.31.0)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.8.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading websockets-10.4-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.8/106.8 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting msgpack==1.0.3 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading msgpack-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.7/323.7 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.9.5)\n",
            "Requirement already satisfied: PyYAML==6.0.1 in /usr/local/lib/python3.10/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0.1)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (24.0)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (67.7.2)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (2024.2.2)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (42.0.7)\n",
            "Collecting aiodns>=1.1.1 (from ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading aiodns-3.2.0-py3-none-any.whl (5.7 kB)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.10/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (1.9.4)\n",
            "Collecting pyluach (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2024.1)\n",
            "Collecting korean-lunar-calendar (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: SQLAlchemy>=1.2.8 in /usr/local/lib/python3.10/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (2.0.30)\n",
            "Collecting thriftpy2>=0.3.9 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading thriftpy2-0.5.0.tar.gz (779 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.0/779.0 kB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.7.1)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2023.4)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.11.4)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.1)\n",
            "Collecting empyrical>=0.5.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading empyrical-0.5.5.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /usr/local/lib/python3.10/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (1.3.4)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.14.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (4.19.2)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (3.20.3)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.4.1)\n",
            "Collecting aiohttp-cors (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
            "Collecting colorful (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting py-spy>=0.2.0 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading py_spy-0.3.14-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencensus (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.7.1)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.20.0)\n",
            "Requirement already satisfied: smart-open in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (6.4.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading virtualenv-20.26.2-py3-none-any.whl (3.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m77.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.64.0)\n",
            "Collecting memray (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading memray-1.12.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboardX>=1.9 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (14.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2023.6.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.5.0)\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.3.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.2.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.8.0.76)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.5.2)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.15.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.66.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (13.7.1)\n",
            "Collecting shimmy[atari]~=1.3.0 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading Shimmy-1.3.0-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (9.4.0)\n",
            "Collecting autorom[accept-rom-license]~=0.6.1 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
            "Collecting numpy>=1.11.1 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas>=0.18.1 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting psycopg2-binary<2.10,>=2.9 (from wrds<4,>=3->finrl==0.3.6)\n",
            "  Downloading psycopg2_binary-2.9.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=0.14.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.9.4)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.2.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (2.4.4)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (3.17.5)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (1.1)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git#egg=elegantrl->finrl==0.3.6) (0.25.2)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading pycares-4.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.7/288.7 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (6.0.5)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (4.0.3)\n",
            "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=2.6.1->ccxt<4,>=3->finrl==0.3.6) (1.16.0)\n",
            "Requirement already satisfied: osqp>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (0.6.2.post8)\n",
            "Requirement already satisfied: ecos>=2 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (2.0.13)\n",
            "Requirement already satisfied: scs>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6) (3.2.4.post1)\n",
            "Requirement already satisfied: pandas-datareader>=0.2 in /usr/local/lib/python3.10/dist-packages (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance<0.3,>=0.2->finrl==0.3.6) (0.5.1)\n",
            "Collecting jedi>=0.16 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.43)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.8.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (2.18.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.7)\n",
            "Collecting ale-py~=0.8.1 (from shimmy[atari]~=1.3.0->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6) (3.0.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.3)\n",
            "Collecting ply<4.0,>=3.4 (from thriftpy2>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "OA9PXZ2Ccpnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Other imports\n",
        "\n",
        "import scipy as sp\n",
        "import math\n",
        "import pandas as pd\n",
        "import requests\n",
        "import json\n",
        "import matplotlib.dates as mdates\n",
        "import numpy as np\n",
        "import pickle\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools as itt\n",
        "import numbers\n",
        "import datetime\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import datetime, timedelta\n",
        "from talib.abstract import *\n",
        "from binance.client import Client\n",
        "from pandas.testing import assert_frame_equal\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "from itertools import combinations\n",
        "from abc import abstractmethod\n",
        "from typing import Iterable, Tuple, List\n",
        "\n",
        "#from google.colab import files"
      ],
      "metadata": {
        "id": "FRaRl37NbOpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot settings\n",
        "\n",
        "SCALE_FACTOR = 1\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "plt.rcParams['figure.figsize'] = [5 * SCALE_FACTOR, 2 * SCALE_FACTOR]\n",
        "plt.rcParams['figure.dpi'] = 300 * SCALE_FACTOR\n",
        "plt.rcParams['font.size'] = 5 * SCALE_FACTOR\n",
        "plt.rcParams['axes.labelsize'] = 5 * SCALE_FACTOR\n",
        "plt.rcParams['axes.titlesize'] = 6 * SCALE_FACTOR\n",
        "plt.rcParams['xtick.labelsize'] = 4 * SCALE_FACTOR\n",
        "plt.rcParams['ytick.labelsize'] = 4 * SCALE_FACTOR\n",
        "plt.rcParams['font.family'] = 'serif'"
      ],
      "metadata": {
        "id": "vywKmrJ0bQFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This requires Binance API keys\n",
        "\n",
        "Video of how to get them easily:\n",
        "\n",
        "https://www.youtube.com/watch?v=qg-oboAY8rM"
      ],
      "metadata": {
        "id": "dzCD3zmCcrOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your Binance data API keys!\n",
        "\n",
        "if not 'API_KEY_Binance' in locals():\n",
        "  print('Please enter your main API key:')\n",
        "  # API_KEY_Binance = input()\n",
        "  API_KEY_Binance = 'qJHiV64YMnIAxA1nQFJOqJf8I9ZHaSfex44EMwLARiWHDGarV9vnvGRJ6na3K6Dp'\n",
        "\n",
        "  print('Please enter your secret API key:')\n",
        "  # API_SECRET_Binance = input()\n",
        "  API_SECRET_Binance = 'HS3c4TjhLEmMA4U7vGPS6poADyOX32V57jfkqwKOL6cwSz3Ikld6YHxQXQDO51t8'"
      ],
      "metadata": {
        "id": "Aml-mqY5bRbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_features_for_each_coin(tic_df):\n",
        "  tic_df['rsi'] = RSI(tic_df['close'], timeperiod=14)\n",
        "  tic_df['macd'], tic_df['macd_signal'], tic_df['macd_hist'] = MACD(tic_df['close'], fastperiod=12,\n",
        "                                                                    slowperiod=26, signalperiod=9)\n",
        "  tic_df['cci'] = CCI(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
        "  tic_df['dx'] = DX(tic_df['high'], tic_df['low'], tic_df['close'], timeperiod=14)\n",
        "  return tic_df"
      ],
      "metadata": {
        "id": "hkcnv_DcbUmb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get data"
      ],
      "metadata": {
        "id": "dJP3-VGIdEwv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BinanceProcessor():\n",
        "    def __init__(self, api_key_binance, api_secret_binance):\n",
        "        self.binance_api_key = api_key_binance  # Enter your own API-key here\n",
        "        self.binance_api_secret = api_secret_binance  # Enter your own API-secret here\n",
        "        self.binance_client = Client(api_key=api_key_binance, api_secret=api_secret_binance)\n",
        "\n",
        "    def run(self, ticker_list, start_date, end_date, time_interval, technical_indicator_list, if_vix):\n",
        "        data = self.download_data(ticker_list, start_date, end_date, time_interval)\n",
        "        data = self.clean_data(data)\n",
        "        data = self.add_technical_indicator(data, technical_indicator_list)\n",
        "        data.index = data['time']\n",
        "\n",
        "        if if_vix:\n",
        "            data = self.add_vix(data)\n",
        "\n",
        "        price_array, tech_array, turbulence_array, time_array = self.df_to_array(data, if_vix)\n",
        "\n",
        "        tech_nan_positions = np.isnan(tech_array)\n",
        "        tech_array[tech_nan_positions] = 0\n",
        "\n",
        "        return data\n",
        "\n",
        "    # main functions\n",
        "    def download_data(self, ticker_list, start_date, end_date,\n",
        "                      time_interval):\n",
        "\n",
        "        self.start_time = start_date\n",
        "        self.end_time = end_date\n",
        "        self.interval = time_interval\n",
        "        self.ticker_list = ticker_list\n",
        "\n",
        "        final_df = pd.DataFrame()\n",
        "        for i in ticker_list:\n",
        "            hist_data = self.get_binance_bars(self.start_time, self.end_time, self.interval, symbol=i)\n",
        "            df = hist_data.iloc[:-1]\n",
        "            df = df.dropna()\n",
        "            df['tic'] = i\n",
        "            final_df = final_df.append(df)\n",
        "\n",
        "        return final_df\n",
        "\n",
        "    def clean_data(self, df):\n",
        "        df = df.dropna()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_technical_indicator(self, df, tech_indicator_list):\n",
        "        # print('Adding self-defined technical indicators is NOT supported yet.')\n",
        "        # print('Use default: MACD, RSI, CCI, DX.')\n",
        "        self.tech_indicator_list = ['open', 'high', 'low', 'close', 'volume',\n",
        "                                    'macd', 'macd_signal', 'macd_hist',\n",
        "                                    'rsi', 'cci', 'dx']\n",
        "\n",
        "        final_df = pd.DataFrame()\n",
        "        for i in df.tic.unique():\n",
        "\n",
        "            # use massive function in previous cell\n",
        "            coin_df = df[df.tic == i].copy()\n",
        "            coin_df = get_features_for_each_coin(coin_df)\n",
        "\n",
        "            # Append constructed tic_df\n",
        "            final_df = final_df.append(coin_df)\n",
        "        return final_df\n",
        "\n",
        "    def add_turbulence(self, df):\n",
        "        print('Turbulence not supported yet. Return original DataFrame.')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def add_vix(self, df):\n",
        "        print('VIX is not applicable for cryptocurrencies. Return original DataFrame')\n",
        "\n",
        "        return df\n",
        "\n",
        "    def df_to_array(self, df, if_vix):\n",
        "        unique_ticker = df.tic.unique()\n",
        "        if_first_time = True\n",
        "        for tic in unique_ticker:\n",
        "            if if_first_time:\n",
        "                price_array = df[df.tic == tic][['close']].values\n",
        "                tech_array = df[df.tic == tic][self.tech_indicator_list].values\n",
        "                if_first_time = False\n",
        "            else:\n",
        "                price_array = np.hstack([price_array, df[df.tic == tic][['close']].values])\n",
        "                tech_array = np.hstack([tech_array, df[df.tic == tic][self.tech_indicator_list].values])\n",
        "\n",
        "            time_array = df[df.tic == self.ticker_list[0]]['time'].values\n",
        "\n",
        "        assert price_array.shape[0] == tech_array.shape[0]\n",
        "\n",
        "        return price_array, tech_array, np.array([]), time_array#\n",
        "\n",
        "    # helper functions\n",
        "    def stringify_dates(self, date: datetime):\n",
        "        return str(int(date.timestamp() * 1000))\n",
        "\n",
        "    def get_binance_bars(self, start_date, end_date, kline_size, symbol):\n",
        "        data_df = pd.DataFrame()\n",
        "        klines = self.binance_client.get_historical_klines(symbol, kline_size, start_date, end_date)\n",
        "        data = pd.DataFrame(klines,\n",
        "                            columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'close_time', 'quote_av',\n",
        "                                     'trades', 'tb_base_av', 'tb_quote_av', 'ignore'])\n",
        "        data = data.drop(labels=['close_time', 'quote_av', 'trades', 'tb_base_av', 'tb_quote_av', 'ignore'], axis=1)\n",
        "        if len(data_df) > 0:\n",
        "            temp_df = pd.DataFrame(data)\n",
        "            data_df = data_df.append(temp_df)\n",
        "        else:\n",
        "            data_df = data\n",
        "\n",
        "        data_df = data_df.apply(pd.to_numeric, errors='coerce')\n",
        "        data_df['time'] = [datetime.fromtimestamp(x / 1000.0) for x in data_df.timestamp]\n",
        "        data.drop(labels=[\"timestamp\"], axis=1)\n",
        "        data_df.index = [x for x in range(len(data_df))]\n",
        "\n",
        "        return data_df\n"
      ],
      "metadata": {
        "id": "br8spZ1wbXYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set constants"
      ],
      "metadata": {
        "id": "kZyTFb60blRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set constants:\n",
        "\n",
        "ticker_list = ['BTCUSDT'\n",
        "              ]\n",
        "\n",
        "\n",
        "time_interval = '1d'\n",
        "\n",
        "# Care format\n",
        "start_date = '2015-01-01 00:00:00'\n",
        "end_date = '2020-01-01 00:00:00'\n",
        "\n",
        "\n",
        "technical_indicator_list = ['open',\n",
        "                             'high',\n",
        "                             'low',\n",
        "                             'close',\n",
        "                             'volume',\n",
        "                             'macd',\n",
        "                             'macd_signal',\n",
        "                             'macd_hist',\n",
        "                             'rsi',\n",
        "                             'cci',\n",
        "                             'dx'\n",
        "                             ]\n",
        "\n",
        "if_vix = False"
      ],
      "metadata": {
        "id": "wE4-QHXYbYQW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data using unified data processor\n",
        "\n",
        "DP = BinanceProcessor(API_KEY_Binance, API_SECRET_Binance)\n",
        "data_ohlcv = DP.run(ticker_list,\n",
        "                    start_date,\n",
        "                    end_date,\n",
        "                    time_interval,\n",
        "                    technical_indicator_list,\n",
        "                    if_vix)"
      ],
      "metadata": {
        "id": "ot2C0ItubZQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop unecessary columns\n",
        "\n",
        "if 'timestamp' in data_ohlcv:\n",
        "  data_ohlcv.drop('timestamp', inplace=True, axis=1)\n",
        "\n",
        "if 'time' in data_ohlcv:\n",
        "  data_ohlcv.drop('time', inplace=True, axis=1)\n",
        "\n",
        "data_ohlcv.head(3)"
      ],
      "metadata": {
        "id": "yl1W84f1baW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Triple barrier method\n",
        "\n",
        "I made a previous medium article about this, if you want to understand it please refer to my previous medium article.\n",
        "\n",
        "However, it is not required to understand PurgedKFoldCV, you can skip on to the next section: PurgedKFoldCV =)\n",
        "\n",
        "https://medium.com/coinmonks/crypto-feature-importance-for-deep-reinforcement-learning-38416616c2a36-8416616c2a36\n"
      ],
      "metadata": {
        "id": "Jr7mFKxRdbUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTANT: Make sure that pd.Timedelta() is according to the time_interval to get the volatility for that time interval\n",
        "\n",
        "if time_interval == '5m':\n",
        "  Delta = pd.Timedelta(minutes=5)\n",
        "elif time_interval == '1h':\n",
        "  Delta = pd.Timedelta(hours=1)\n",
        "elif time_interval == '1d':\n",
        "  Delta = pd.Timedelta(days=1)\n",
        "else:\n",
        "  raise ValueError('Timeframe not supported yet, please manually add!')"
      ],
      "metadata": {
        "id": "TPMJCMx3dwjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vol(prices, span=100, delta=Delta):\n",
        "\n",
        "  # 1. compute returns of the form p[t]/p[t-1] - 1\n",
        "  # 1.1 find the timestamps of p[t-1] values\n",
        "  df0 = prices.index.searchsorted(prices.index - delta)\n",
        "  df0 = df0[df0 > 0]\n",
        "\n",
        "  # 1.2 align timestamps of p[t-1] to timestamps of p[t]\n",
        "  df0 = pd.Series(prices.index[df0-1],\n",
        "           index=prices.index[prices.shape[0]-df0.shape[0] : ])\n",
        "\n",
        "  # 1.3 get values by timestamps, then compute returns\n",
        "  df0 = prices.loc[df0.index] / prices.loc[df0.values].values - 1\n",
        "\n",
        "  # 2. estimate rolling standard deviation\n",
        "  df0 = df0.ewm(span=span).std()\n",
        "\n",
        "  return df0"
      ],
      "metadata": {
        "id": "hqqNxQFIdxBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_ohlcv = data_ohlcv.assign(volatility=get_vol(data_ohlcv.close)).dropna()"
      ],
      "metadata": {
        "id": "XhpZGD7VdzRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_barriers():\n",
        "\n",
        "  #create a container\n",
        "  barriers = pd.DataFrame(columns=['datapoints_passed',\n",
        "            'price', 'vert_barrier', \\\n",
        "            'top_barrier', 'bottom_barrier'], \\\n",
        "              index = daily_volatility.index)\n",
        "\n",
        "  for datapoint, vol in daily_volatility.iteritems():\n",
        "\n",
        "    datapoints_passed = len(daily_volatility.loc \\\n",
        "                  [daily_volatility.index[0] : datapoint])\n",
        "\n",
        "    #set the vertical barrier\n",
        "    if (datapoints_passed + t_final < len(daily_volatility.index) \\\n",
        "        and t_final != 0):\n",
        "        vert_barrier = daily_volatility.index[\n",
        "                            datapoints_passed + t_final]\n",
        "    else:\n",
        "        vert_barrier = np.nan\n",
        "\n",
        "    #set the top barrier\n",
        "    if upper_lower_multipliers[0] > 0:\n",
        "        top_barrier = prices.loc[datapoint] + prices.loc[datapoint] * \\\n",
        "                      upper_lower_multipliers[0] * vol\n",
        "    else:\n",
        "        #set it to NaNs\n",
        "        top_barrier = pd.Series(index=prices.index)\n",
        "\n",
        "    #set the bottom barrier\n",
        "    if upper_lower_multipliers[1] > 0:\n",
        "        bottom_barrier = prices.loc[datapoint] - prices.loc[datapoint] * \\\n",
        "                      upper_lower_multipliers[1] * vol\n",
        "    else:\n",
        "        #set it to NaNs\n",
        "        bottom_barrier = pd.Series(index=prices.index)\n",
        "\n",
        "    barriers.loc[datapoint, ['datapoints_passed', 'price', 'vert_barrier','top_barrier', 'bottom_barrier']] = \\\n",
        "    datapoints_passed, prices.loc[datapoint], vert_barrier, top_barrier, bottom_barrier\n",
        "\n",
        "  return barriers"
      ],
      "metadata": {
        "id": "ZjaI2GDrd0Nr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set barrier parameters\n",
        "\n",
        "daily_volatility = data_ohlcv['volatility']\n",
        "t_final = 10\n",
        "upper_lower_multipliers = [2, 2]\n",
        "price = data_ohlcv['close']\n",
        "prices = price[daily_volatility.index]"
      ],
      "metadata": {
        "id": "SKWhNCGBd0o4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "barriers = get_barriers()\n",
        "barriers.head(5)"
      ],
      "metadata": {
        "id": "DgqsdrjPd2BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_labels():\n",
        "  barriers[\"label_barrier\"] = None\n",
        "  for i in range(len(barriers.index)):\n",
        "    start = barriers.index[i]\n",
        "    end = barriers.vert_barrier[i]\n",
        "    if pd.notna(end):\n",
        "\n",
        "        # assign the initial and final price\n",
        "        price_initial = barriers.price[start]\n",
        "        price_final = barriers.price[end]\n",
        "\n",
        "        # assign the top and bottom barriers\n",
        "        top_barrier = barriers.top_barrier[i]\n",
        "        bottom_barrier = barriers.bottom_barrier[i]\n",
        "\n",
        "        #set the profit taking and stop loss conditons\n",
        "        condition_pt = (barriers.price[start: end] >= \\\n",
        "          top_barrier).any()\n",
        "        condition_sl = (barriers.price[start: end] <= \\\n",
        "          bottom_barrier).any()\n",
        "\n",
        "        #assign the labels\n",
        "        if condition_pt:\n",
        "            barriers['label_barrier'][i] = 2\n",
        "        elif condition_sl:\n",
        "            barriers['label_barrier'][i] = 0\n",
        "        else:\n",
        "          barriers['label_barrier'][i] = 1\n",
        "  return"
      ],
      "metadata": {
        "id": "3o1h3zZ8d3Ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use function to produce barriers\n",
        "\n",
        "get_labels()\n",
        "barriers\n",
        "\n",
        "# Merge the barriers with the main dataset and drop the last t_final + 1 barriers (as they are too close to the end)\n",
        "\n",
        "data_ohlcv = data_ohlcv.merge(barriers[['vert_barrier', 'top_barrier', 'bottom_barrier', 'label_barrier']], left_on='time', right_on='time')\n",
        "data_ohlcv.drop(data_ohlcv.tail(t_final + 1).index,inplace = True)\n",
        "data_ohlcv = data_ohlcv.drop(['vert_barrier', 'top_barrier', 'bottom_barrier','tic'], axis = 1)\n",
        "data_ohlcv.head(5)"
      ],
      "metadata": {
        "id": "GE-s2pGHd4YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Combinatorial PurgedKFoldCV"
      ],
      "metadata": {
        "id": "PNNUXedJd4s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseTimeSeriesCrossValidator:\n",
        "    \"\"\"\n",
        "    Abstract class for time series cross-validation.\n",
        "    Time series cross-validation requires each sample has a prediction time pred_time, at which the features are used to\n",
        "    predict the response, and an evaluation time eval_time, at which the response is known and the error can be\n",
        "    computed. Importantly, it means that unlike in standard sklearn cross-validation, the samples X, response y,\n",
        "    pred_times and eval_times must all be pandas dataframe/series having the same index. It is also assumed that the\n",
        "    samples are time-ordered with respect to the prediction time (i.e. pred_times is non-decreasing).\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10):\n",
        "        if not isinstance(n_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of folds must be of Integral type. {n_splits} of type {type(n_splits)}\"\n",
        "                             f\" was passed.\")\n",
        "        n_splits = int(n_splits)\n",
        "        if n_splits <= 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting n_splits = 2 \"\n",
        "                             f\"or more, got n_splits = {n_splits}.\")\n",
        "        self.n_splits = n_splits\n",
        "        self.pred_times = None\n",
        "        self.eval_times = None\n",
        "        self.indices = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None):\n",
        "        if not isinstance(X, pd.DataFrame) and not isinstance(X, pd.Series):\n",
        "            raise ValueError('X should be a pandas DataFrame/Series.')\n",
        "        if not isinstance(y, pd.Series) and y is not None:\n",
        "            raise ValueError('y should be a pandas Series.')\n",
        "        if not isinstance(pred_times, pd.Series):\n",
        "            raise ValueError('pred_times should be a pandas Series.')\n",
        "        if not isinstance(eval_times, pd.Series):\n",
        "            raise ValueError('eval_times should be a pandas Series.')\n",
        "        if y is not None and (X.index == y.index).sum() != len(y):\n",
        "            raise ValueError('X and y must have the same index')\n",
        "        if (X.index == pred_times.index).sum() != len(pred_times):\n",
        "            raise ValueError('X and pred_times must have the same index')\n",
        "        if (X.index == eval_times.index).sum() != len(eval_times):\n",
        "            raise ValueError('X and eval_times must have the same index')\n",
        "\n",
        "        if not pred_times.equals(pred_times.sort_values()):\n",
        "            raise ValueError('pred_times should be sorted')\n",
        "        if not eval_times.equals(eval_times.sort_values()):\n",
        "            raise ValueError('eval_times should be sorted')\n",
        "\n",
        "        self.pred_times = pred_times\n",
        "        self.eval_times = eval_times\n",
        "        self.indices = np.arange(X.shape[0])\n",
        "\n",
        "class CombPurgedKFoldCVLocal(BaseTimeSeriesCrossValidator):\n",
        "    \"\"\"\n",
        "    Purged and embargoed combinatorial cross-validation\n",
        "    As described in Advances in financial machine learning, Marcos Lopez de Prado, 2018.\n",
        "    The samples are decomposed into n_splits folds containing equal numbers of samples, without shuffling. In each cross\n",
        "    validation round, n_test_splits folds are used as the test set, while the other folds are used as the train set.\n",
        "    There are as many rounds as n_test_splits folds among the n_splits folds.\n",
        "    Each sample should be tagged with a prediction time pred_time and an evaluation time eval_time. The split is such\n",
        "    that the intervals [pred_times, eval_times] associated to samples in the train and test set do not overlap. (The\n",
        "    overlapping samples are dropped.) In addition, an \"embargo\" period is defined, giving the minimal time between an\n",
        "    evaluation time in the test set and a prediction time in the training set. This is to avoid, in the presence of\n",
        "    temporal correlation, a contamination of the test set by the train set.\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_splits : int, default=10\n",
        "        Number of folds. Must be at least 2.\n",
        "    n_test_splits : int, default=2\n",
        "        Number of folds used in the test set. Must be at least 1.\n",
        "    embargo_td : pd.Timedelta, default=0\n",
        "        Embargo period (see explanations above).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_splits=10, n_test_splits=2, embargo_td=pd.Timedelta(minutes=0)):\n",
        "        super().__init__(n_splits)\n",
        "        if not isinstance(n_test_splits, numbers.Integral):\n",
        "            raise ValueError(f\"The number of test folds must be of Integral type. {n_test_splits} of type \"\n",
        "                             f\"{type(n_test_splits)} was passed.\")\n",
        "        n_test_splits = int(n_test_splits)\n",
        "        if n_test_splits <= 0 or n_test_splits > self.n_splits - 1:\n",
        "            raise ValueError(f\"K-fold cross-validation requires at least one train/test split by setting \"\n",
        "                             f\"n_test_splits between 1 and n_splits - 1, got n_test_splits = {n_test_splits}.\")\n",
        "        self.n_test_splits = n_test_splits\n",
        "        if not isinstance(embargo_td, pd.Timedelta):\n",
        "            raise ValueError(f\"The embargo time should be of type Pandas Timedelta. {embargo_td} of type \"\n",
        "                             f\"{type(embargo_td)} was passed.\")\n",
        "        if embargo_td < pd.Timedelta(minutes=0):\n",
        "            raise ValueError(f\"The embargo time should be positive, got embargo = {embargo_td}.\")\n",
        "        self.embargo_td = embargo_td\n",
        "\n",
        "    def split(self, X: pd.DataFrame, y: pd.Series = None,\n",
        "              pred_times: pd.Series = None, eval_times: pd.Series = None) -> Iterable[Tuple[np.ndarray, np.ndarray]]:\n",
        "        \"\"\"\n",
        "        Yield the indices of the train and test sets.\n",
        "        Although the samples are passed in the form of a pandas dataframe, the indices returned are position indices,\n",
        "        not labels.\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : pd.DataFrame, shape (n_samples, n_features), required\n",
        "            Samples. Only used to extract n_samples.\n",
        "        y : pd.Series, not used, inherited from _BaseKFold\n",
        "        pred_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which predictions are made. pred_times.index has to coincide with X.index.\n",
        "        eval_times : pd.Series, shape (n_samples,), required\n",
        "            Times at which the response becomes available and the error can be computed. eval_times.index has to\n",
        "            coincide with X.index.\n",
        "        Returnst\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        \"\"\"\n",
        "        super().split(X, y, pred_times, eval_times)\n",
        "\n",
        "        # Fold boundaries\n",
        "        fold_bounds = [(fold[0], fold[-1] + 1) for fold in np.array_split(self.indices, self.n_splits)]\n",
        "        # List of all combinations of n_test_splits folds selected to become test sets\n",
        "        selected_fold_bounds = list(itt.combinations(fold_bounds, self.n_test_splits))\n",
        "        # In order for the first round to have its whole test set at the end of the dataset\n",
        "        selected_fold_bounds.reverse()\n",
        "\n",
        "        for fold_bound_list in selected_fold_bounds:\n",
        "            # Computes the bounds of the test set, and the corresponding indices\n",
        "            test_fold_bounds, test_indices = self.compute_test_set(fold_bound_list)\n",
        "            # Computes the train set indices\n",
        "            train_indices = self.compute_train_set(test_fold_bounds, test_indices)\n",
        "\n",
        "            yield train_indices, test_indices\n",
        "\n",
        "    def compute_train_set(self, test_fold_bounds: List[Tuple[int, int]], test_indices: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Compute the position indices of samples in the train set.\n",
        "        Parameters\n",
        "        ----------\n",
        "        test_fold_bounds : List of tuples of position indices\n",
        "            Each tuple records the bounds of a block of indices in the test set.\n",
        "        test_indices : np.ndarray\n",
        "            A numpy array containing all the indices in the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        train_indices: np.ndarray\n",
        "            A numpy array containing all the indices in the train set.\n",
        "        \"\"\"\n",
        "        # As a first approximation, the train set is the complement of the test set\n",
        "        train_indices = np.setdiff1d(self.indices, test_indices)\n",
        "        # But we now have to purge and embargo\n",
        "        for test_fold_start, test_fold_end in test_fold_bounds:\n",
        "            # Purge\n",
        "            train_indices = purge(self, train_indices, test_fold_start, test_fold_end)\n",
        "            # Embargo\n",
        "            train_indices = embargo(self, train_indices, test_indices, test_fold_end)\n",
        "        return train_indices\n",
        "\n",
        "    def compute_test_set(self, fold_bound_list: List[Tuple[int, int]]) -> Tuple[List[Tuple[int, int]], np.ndarray]:\n",
        "        \"\"\"\n",
        "        Compute the indices of the samples in the test set.\n",
        "        Parameterst\n",
        "        ----------\n",
        "        fold_bound_list: List of tuples of position indices\n",
        "            Each tuple records the bounds of the folds belonging to the test set.\n",
        "        Returns\n",
        "        -------\n",
        "        test_fold_bounds: List of tuples of position indices\n",
        "            Like fold_bound_list, but witest_fold_boundsth the neighboring folds in the test set merged.\n",
        "        test_indices: np.ndarray\n",
        "            A numpy array containing the test indices.\n",
        "        \"\"\"\n",
        "        test_indices = np.empty(0)\n",
        "        test_fold_bounds = []\n",
        "        for fold_start, fold_end in fold_bound_list:\n",
        "            # Records the boundaries of the current test split\n",
        "            if not test_fold_bounds or fold_start != test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds.append((fold_start, fold_end))\n",
        "            # If the current test split is contiguous to the previous one, simply updates the endpoint\n",
        "            elif fold_start == test_fold_bounds[-1][-1]:\n",
        "                test_fold_bounds[-1] = (test_fold_bounds[-1][0], fold_end)\n",
        "            test_indices = np.union1d(test_indices, self.indices[fold_start:fold_end]).astype(int)\n",
        "        return test_fold_bounds, test_indices\n",
        "\n",
        "\n",
        "def compute_fold_bounds(cv: BaseTimeSeriesCrossValidator, split_by_time: bool) -> List[int]:\n",
        "    \"\"\"\n",
        "    Compute a list containing the fold (left) boundaries.\n",
        "    Parameters\n",
        "    ----------\n",
        "    cv: BaseTimeSeriesCrossValidator\n",
        "        Cross-validation object for which the bounds need to be computed.\n",
        "    split_by_time: bool\n",
        "        If False, the folds contain an (approximately) equal number of samples. If True, the folds span identical\n",
        "        time intervals.\n",
        "    \"\"\"\n",
        "    if split_by_time:\n",
        "        full_time_span = cv.pred_times.max() - cv.pred_times.min()\n",
        "        fold_time_span = full_time_span / cv.n_splits\n",
        "        fold_bounds_times = [cv.pred_times.iloc[0] + fold_time_span * n for n in range(cv.n_splits)]\n",
        "        return cv.pred_times.searchsorted(fold_bounds_times)\n",
        "    else:\n",
        "        return [fold[0] for fold in np.array_split(cv.indices, cv.n_splits)]\n",
        "\n",
        "\n",
        "def embargo(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "            test_indices: np.ndarray, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply the embargo procedure to part of the train set.\n",
        "    This amounts to dropping the train set samples whose prediction time occurs within self.embargo_dt of the test\n",
        "    set sample evaluation times. This method applies the embargo only to the part of the training set immediately\n",
        "    following the end of the test set determined by test_fold_end.\n",
        "    Parameters\n",
        "    -------mestamps of p[t-1] values\n",
        "  df0 = prices.inde---\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times, cv.embargo_dt and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_indices : np.ndarray\n",
        "        A numpy array containing all the indices of the samples in the test set.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of a test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        The same array, with the indices subject to embargo removed.\n",
        "    \"\"\"\n",
        "    if not hasattr(cv, 'embargo_td'):\n",
        "        raise ValueError(\"The passed cross-validation object should have a member cv.embargo_td defining the embargo\"\n",
        "                         \"time.\")\n",
        "    last_test_eval_time = cv.eval_times.iloc[test_indices[test_indices <= test_fold_end]].max()\n",
        "    min_train_index = len(cv.pred_times[cv.pred_times <= last_test_eval_time + cv.embargo_td])\n",
        "    if min_train_index < cv.indices.shape[0]:\n",
        "        allowed_indices = np.concatenate((cv.indices[:test_fold_end], cv.indices[min_train_index:]))\n",
        "        train_indices = np.intersect1d(train_indices, allowed_indices)\n",
        "    return train_indices\n",
        "\n",
        "\n",
        "def purge(cv: BaseTimeSeriesCrossValidator, train_indices: np.ndarray,\n",
        "          test_fold_start: int, test_fold_end: int) -> np.ndarray:\n",
        "    \"\"\"data_ohlcv\n",
        "    Purge part of the train set.\n",
        "    Given a left boundary index test_fold_start of the test set, this method removes from the train set all the\n",
        "    samples whose evaluation time is posterior to the prediction time of the first test sample after the boundary.\n",
        "    Parameters\n",
        "    ----------combinatorial purged k fold\n",
        "    cv: Cross-validation class\n",
        "        Needs to have the attributes cv.pred_times, cv.eval_times and cv.indices.\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing all the indices of the samples currently included in the train set.\n",
        "    test_fold_start : int\n",
        "        Index corresponding to the start of a test set block.\n",
        "    test_fold_end : int\n",
        "        Index corresponding to the end of the same test set block.\n",
        "    Returns\n",
        "    -------\n",
        "    train_indices: np.ndarray\n",
        "        A numpy array containing the train indices purged at test_fold_start.\n",
        "    \"\"\"\n",
        "    time_test_fold_start = cv.pred_times.iloc[test_fold_start]\n",
        "    # The train indices before the start of the test fold, purged.\n",
        "    train_indices_1 = np.intersect1d(train_indices, cv.indices[cv.eval_times < time_test_fold_start])\n",
        "    # The train indices after the end of the test fold.\n",
        "    train_indices_2 = np.intersect1d(train_indices, cv.indices[test_fold_end:])\n",
        "    return np.concatenate((train_indices_1, train_indices_2))"
      ],
      "metadata": {
        "id": "6--viSo7i0ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The generator function for the unique paths"
      ],
      "metadata": {
        "id": "VI8JXiu1eotV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_test_paths_generator(t_span, n, k, prediction_times, evaluation_times, verbose=True):\n",
        "    # split data into N groups, with N << T\n",
        "    # this will assign each index position to a group position\n",
        "    group_num = np.arange(t_span) // (t_span // n)\n",
        "    group_num[group_num == n] = n-1\n",
        "\n",
        "    # generate the combinations\n",
        "    test_groups = np.array(list(itt.combinations(np.arange(n), k))).reshape(-1, k)\n",
        "    C_nk = len(test_groups)\n",
        "    n_paths = C_nk * k // n\n",
        "\n",
        "\n",
        "    if verbose:\n",
        "        print('n_sim:', C_nk)\n",
        "        print('n_paths:', n_paths)\n",
        "\n",
        "    # is_test is a T x C(n, k) array where each column is a logical array\n",
        "    # indicating which observation in in the test set\n",
        "    is_test_group = np.full((n, C_nk), fill_value=False)\n",
        "    is_test = np.full((t_span, C_nk), fill_value=False)\n",
        "\n",
        "    # assign test folds for each of the C(n, k) simulations\n",
        "    for k, pair in enumerate(test_groups):\n",
        "        i, j = pair\n",
        "        is_test_group[[i, j], k] = True\n",
        "\n",
        "        # assigning the test folds\n",
        "        mask = (group_num == i) | (group_num == j)\n",
        "        is_test[mask, k] = True\n",
        "\n",
        "    # for each path, connect the folds from different simulations to form a backtest path\n",
        "    # the fold coordinates are: the fold number, and the simulation index e.g. simulation 0, fold 0 etc\n",
        "    path_folds = np.full((n, n_paths), fill_value=np.nan)\n",
        "\n",
        "    for i in range(n_paths):\n",
        "        for j in range(n):\n",
        "            s_idx = is_test_group[j, :].argmax().astype(int)\n",
        "            path_folds[j, i] = s_idx\n",
        "            is_test_group[j, s_idx] = False\n",
        "            cv.split(X, y, pred_times=prediction_times, eval_times=evaluation_times)\n",
        "\n",
        "    # finally, for each path we indicate which simulation we're building the path from and the time indices\n",
        "    paths = np.full((t_span, n_paths), fill_value= np.nan)\n",
        "\n",
        "    for p in range(n_paths):\n",
        "        for i in range(n):\n",
        "            mask = (group_num == i)\n",
        "            paths[mask, p] = int(path_folds[i, p])\n",
        "    # paths = paths_# .astype(int)\n",
        "\n",
        "    return (is_test, paths, path_folds)"
      ],
      "metadata": {
        "id": "MJ9RUhkSeL_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The plotting function for the Combinatorial PurgedKFold\n",
        "\n",
        "Made this based on https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html"
      ],
      "metadata": {
        "id": "cMr24XD2er1a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmap_data = plt.cm.Paired\n",
        "cmap_cv = plt.cm.coolwarm\n",
        "\n",
        "def plot_cv_indices(cv, X, y, group, ax, n_paths, k, paths, lw=5):\n",
        "    \"\"\"Create a sample plot for indices of a cross-validation object.\"\"\"\n",
        "\n",
        "    # generate the combinations\n",
        "    N = n_paths + 1\n",
        "    test_groups = np.array(list(itt.combinations(np.arange(N), k))).reshape(-1, k)\n",
        "    n_splits = len(test_groups)\n",
        "\n",
        "    # Generate the training/testing visualizations for each CV split\n",
        "    for ii, (tr, tt) in enumerate(cv.split(X, y, pred_times=prediction_times, eval_times=evaluation_times)):\n",
        "\n",
        "        # print('fold', ii, '\\n')\n",
        "        # print(tr, '\\n')\n",
        "        # print(tt, '\\n')\n",
        "\n",
        "        # Fill in indices with the training/test groups\n",
        "        indices = np.array([np.nan] * len(X))\n",
        "        indices[tt] = 1\n",
        "        indices[tr] = 0\n",
        "        indices[np.isnan(indices)] = 2\n",
        "\n",
        "        # Visualize the results\n",
        "        ax.scatter(\n",
        "            [ii + 0.5] * len(indices),\n",
        "            range(len(indices)),\n",
        "            c=[indices],\n",
        "            marker=\"_\",\n",
        "            lw=lw,\n",
        "            cmap=cmap_cv,\n",
        "            vmin=-0.2,\n",
        "            vmax=1.2\n",
        "        )\n",
        "\n",
        "    # Plot the data classes and groups at the end\n",
        "    ax.scatter(\n",
        "        [ii + 1.5] * len(X),\n",
        "        range(len(X)),\n",
        "        c=y,\n",
        "        marker=\"_\",\n",
        "        lw=lw,\n",
        "        cmap=cmap_data\n",
        "    )\n",
        "\n",
        "    ax.scatter(\n",
        "        [ii + 2.5] * len(X),\n",
        "        range(len(X)),\n",
        "        c=group,\n",
        "        marker=\"_\",\n",
        "        lw=lw,\n",
        "        cmap=cmap_data\n",
        "    )\n",
        "\n",
        "    # Formatting\n",
        "    xlabelz = list(range(n_splits, 0 , -1))\n",
        "    xlabelz = ['S' + str(x) for x in xlabelz]\n",
        "    xticklabels = xlabelz + [\"class\", \"group\"]\n",
        "\n",
        "    ax.set(\n",
        "        xticks=np.arange(n_splits + 2) + 0.45,\n",
        "        xticklabels=xticklabels,\n",
        "        ylabel=\"Sample index\",\n",
        "        xlabel=\"CV iteration\",\n",
        "        xlim=[n_splits + 2.2, -0.2],\n",
        "        ylim=[0, X.shape[0]],\n",
        "    )\n",
        "    ax.set_title(\"{}\".format(type(cv).__name__), fontsize=5)\n",
        "    ax.xaxis.tick_top()\n",
        "\n",
        "    return ax"
      ],
      "metadata": {
        "id": "8Qco6-mmeRaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Just setting the constants + timeseriescv installation"
      ],
      "metadata": {
        "id": "SLBYAi72exrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = data_ohlcv\n",
        "\n",
        "data_index = data.index\n",
        "\n",
        "# Train data\n",
        "X = data.drop(['label_barrier'], axis = 1)\n",
        "X.drop(X.tail(t_final).index,inplace = True)\n",
        "\n",
        "# Test data\n",
        "y = data[['label_barrier']]\n",
        "y.reindex(data_index)\n",
        "y = y[:-t_final]\n",
        "y = y.squeeze()\n",
        "\n",
        "# prediction and evalution times\n",
        "t1_ = data.index\n",
        "\n",
        "# recall that we are holding our position for 10 days\n",
        "# normally t1 is important is there events such as stop losses, or take profit events\n",
        "# Recall t_final from before! This is the maximum of a box!!\n",
        "\n",
        "# prediction time is moment of observationxticklabels\n",
        "prediction_times = pd.Series(t1_[:-t_final], index = X.index)\n",
        "\n",
        "# evaluation time is moment of evaluation event\n",
        "evaluation_times = pd.Series(t1_[t_final:], index = X.index)"
      ],
      "metadata": {
        "id": "NMRv7V_9eSWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_paths = 5\n",
        "k = 2\n",
        "N = num_paths + 1\n",
        "embargo_td = Delta * t_final * 2\n",
        "cv = CombPurgedKFoldCVLocal(n_splits=N, n_test_splits=k, embargo_td=embargo_td)\n",
        "\n",
        "# Compute backtest paths\n",
        "_, paths, _= back_test_paths_generator(X.shape[0], N, k, prediction_times, evaluation_times)\n",
        "\n",
        "# Plot PurgedKFold split\n",
        "groups = list(range(X.shape[0]))\n",
        "fig, ax = plt.subplots()\n",
        "plot_cv_indices(cv, X, y, groups, ax, num_paths, k, paths)\n",
        "plt.gca().invert_yaxis()"
      ],
      "metadata": {
        "id": "piFfU3SBeUaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paths example"
      ],
      "metadata": {
        "id": "zcf2cHw1jE3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def back_test_paths_generator(t_span, n, k, prediction_times, evaluation_times, verbose=True):\n",
        "    # split data into N groups, with N << T\n",
        "    # this will assign each index position to a group position\n",
        "    group_num = np.arange(t_span) // (t_span // n)\n",
        "    group_num[group_num == n] = n-1\n",
        "\n",
        "    # generate the combinations\n",
        "    test_groups = np.array(list(itt.combinations(np.arange(n), k))).reshape(-1, k)\n",
        "    C_nk = len(test_groups)\n",
        "    n_paths = C_nk * k // n\n",
        "\n",
        "    print(n_paths)\n",
        "\n",
        "    if verbose:\n",
        "        print('n_sim:', C_nk)\n",
        "        print('n_paths:', n_paths)\n",
        "\n",
        "    # is_test is a T x C(n, k) array where each column is a logical array\n",
        "    # indicating which observation in in the test set\n",
        "    is_test_group = np.full((n, C_nk), fill_value=False)\n",
        "    is_test = np.full((t_span, C_nk), fill_value=False)\n",
        "\n",
        "    # assign test folds for each of the C(n, k) simulations\n",
        "    for k, pair in enumerate(test_groups):\n",
        "        i, j = pair\n",
        "        is_test_group[[i, j], k] = True\n",
        "\n",
        "        # assigning the test folds\n",
        "        mask = (group_num == i) | (group_num == j)\n",
        "        is_test[mask, k] = True\n",
        "\n",
        "    # for each path, connect the folds from different simulations to form a backtest path\n",
        "    # the fold coordinates are: the fold number, and the simulation index e.g. simulation 0, fold 0 etc\n",
        "    path_folds = np.full((n, n_paths), fill_value=np.nan)\n",
        "\n",
        "    for i in range(n_paths):\n",
        "        for j in range(n):\n",
        "            s_idx = is_test_group[j, :].argmax().astype(int)\n",
        "            path_folds[j, i] = s_idx\n",
        "            is_test_group[j, s_idx] = False\n",
        "            cv.split(X, y, pred_times=prediction_times, eval_times=evaluation_times)\n",
        "\n",
        "    # finally, for each path we indicate which simulation we're building the path from and the time indices\n",
        "    paths = np.full((t_span, n_paths), fill_value= np.nan)\n",
        "\n",
        "    for p in range(n_paths):\n",
        "        for i in range(n):\n",
        "            mask = (group_num == i)\n",
        "            paths[mask, p] = int(path_folds[i, p])\n",
        "    # paths = paths_# .astype(int)\n",
        "\n",
        "    return (is_test, paths, path_folds)"
      ],
      "metadata": {
        "id": "8OLHBM1gpVlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute backtest paths\n",
        "_, paths, _= back_test_paths_generator(30, 6, k, prediction_times, evaluation_times)\n",
        "paths + 1"
      ],
      "metadata": {
        "id": "ZGEkk6rzjHIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "paths.shape"
      ],
      "metadata": {
        "id": "UKN6a8JDoWZb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}